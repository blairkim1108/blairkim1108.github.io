[
  {
    "objectID": "finalpresentation.html#introduction",
    "href": "finalpresentation.html#introduction",
    "title": "final presentation",
    "section": "Introduction",
    "text": "Introduction\n\nIn 2014, Facebook (now Meta) ran a large‑scale psychological experiment on 689,003 users, altering their News Feed to test whether emotions on social media are contagious (Kramer, Guillory & Hancock, 2014).\nEmotional contagion had previously been studied in person—e.g., the Framingham Heart Study tracked 4,739 individuals over 20 years and found that happiness spreads through social networks (Fowler & Christakis, 2008).\nWhile pioneering in the digital realm, Facebook’s study raises serious ethical questions about how users were informed and protected."
  },
  {
    "objectID": "finalpresentation.html#data-ethics-principles",
    "href": "finalpresentation.html#data-ethics-principles",
    "title": "final presentation",
    "section": "Data Ethics Principles",
    "text": "Data Ethics Principles\nfrom Data Values and Principles manifesto\nAs data teams, we aim to…\n\nUse data to improve life for our users, customers, organizations, and communities.\nCreate reproducible and extensible work.\nRecognize and mitigate bias in ourselves and in the data we use.\nConsider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society.\nProtect the privacy and security of individuals represented in our data."
  },
  {
    "objectID": "finalpresentation.html#about-the-study",
    "href": "finalpresentation.html#about-the-study",
    "title": "final presentation",
    "section": "About the Study",
    "text": "About the Study\n\nMethod: Randomly assigned users to one of three groups—positive‑emotion, negative‑emotion, or control—and manipulated the emotional content in their News Feed.\nMeasure: Collected and analyzed subsequent status updates to quantify changes in users’ emotional expression.\nResults: Users exposed to fewer positive posts wrote more negative updates (and vice versa), demonstrating online emotional contagion."
  },
  {
    "objectID": "finalpresentation.html#data-ethics-concern-consent-structure",
    "href": "finalpresentation.html#data-ethics-concern-consent-structure",
    "title": "final presentation",
    "section": "Data Ethics Concern: Consent Structure",
    "text": "Data Ethics Concern: Consent Structure\n\nClaimed “consent” via Facebook’s Data Use Policy, rather than through informed‐consent procedures standard in psychological research.\nNo participant briefing on study aims, potential harms, right to withdraw, or debriefing and support for emotional distress.\nConducted before “research” was explicitly added to the policy; likely included minors without age screening."
  },
  {
    "objectID": "finalpresentation.html#data-ethics-concern-anonymity-of-data",
    "href": "finalpresentation.html#data-ethics-concern-anonymity-of-data",
    "title": "final presentation",
    "section": "Data Ethics Concern: Anonymity of data",
    "text": "Data Ethics Concern: Anonymity of data\n\nParticipants identified only by user ID, but these IDs link to personal profiles and Messenger chats.\nFacebook researchers’ internal access could re‑identify individuals, compromising true anonymity."
  },
  {
    "objectID": "finalpresentation.html#data-ethics-concern-representativeness-of-the-sample",
    "href": "finalpresentation.html#data-ethics-concern-representativeness-of-the-sample",
    "title": "final presentation",
    "section": "Data Ethics Concern: Representativeness of the Sample",
    "text": "Data Ethics Concern: Representativeness of the Sample\n\nSampling by user ID was strictly random, yet no demographic data (age, gender, race) were collected or reported.\nUnknown whether certain groups were over‑ or under‑represented, limiting the study’s generalizability.\nQuestionable whether results can be generalized to people who are not on Facebook"
  },
  {
    "objectID": "finalpresentation.html#data-ethics-concern-accessibility-of-the-data",
    "href": "finalpresentation.html#data-ethics-concern-accessibility-of-the-data",
    "title": "final presentation",
    "section": "Data Ethics Concern: Accessibility of the Data",
    "text": "Data Ethics Concern: Accessibility of the Data\n\nRaw data were not made publicly available, protecting user privacy but preventing reproducibility.\nLack of open access hinders independent verification of analyses and further research building on these findings."
  },
  {
    "objectID": "finalpresentation.html#conclusion",
    "href": "finalpresentation.html#conclusion",
    "title": "final presentation",
    "section": "Conclusion",
    "text": "Conclusion\n\nEmotional contagion in digital space might be real.\nAlthough the findings of the study were pioneering, ethics of this study is problematic.\nBetter data collection practice is important for the integrity of the research and reliability of its findings."
  },
  {
    "objectID": "finalpresentation.html#references",
    "href": "finalpresentation.html#references",
    "title": "final presentation",
    "section": "References",
    "text": "References\n\nFowler, J. H., & Christakis, N. A. (2008). Dynamic spread of happiness in a large social network: longitudinal analysis over 20 years in the Framingham Heart Study. BMJ (Clinical research ed.), 337, a2338. https://doi.org/10.1136/bmj.a2338\nMcNeal, G. S. (2014, July 1). Facebook manipulated user news feeds to create emotional responses. Forbes. https://www.forbes.com/sites/gregorymcneal/2014/06/28/facebook-manipulated-user-news-feeds-to-create-emotional-contagion/\nKramer, A. D., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences of the United States of America, 111(24), 8788–8790. https://doi.org/10.1073/pnas.1320040111\nHow usernames and user ids are used on Facebook Profiles: Facebook help center. How usernames and user IDs are used on Facebook profiles | Facebook Help Center. (n.d.). https://www.facebook.com/help/211813265517027/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Contact\n\n(909) 267‑4560\n\nClaremont, CA\n\nykad2018@pomona.edu\n\nLinkedIn\n\n\n\nEducation\nPomona College, Claremont, CA\nB.A. Psychology (Sep 2018 – May 2025)\nRelevant coursework: Research Design & Methodology, Behavioral Psychology, Data Mining for Psychologists, Intro to Statistics, Intro CS with Python/Machine Learning, Social Psychology, Foundations of 2D Design, Data Science with R\n\n\nProfessional Experience\n\nBAR 21 (Remote)\nProduct Lead (Aug 2024 – Present)\n- Led game development as Product Lead for a 20‑member team, overseeing project planning, sprint coordination, documentation, and backlog prioritization to ensure timely delivery and cross‑team alignment.\n- Spearheaded game design documentation, structuring core mechanics and narrative while researching references to establish a cohesive design framework.\n- Designed in‑game user interface (UI) in collaboration with engineers and game artists to enhance player experience, and led quality assurance efforts by introducing a structured QA process to improve game stability and performance.\n\n\nDaangn Market Inc. (Seoul, South Korea)\nProduct Manager Intern, Advertising Platform (Jan 2024 – Aug 2024)\n- Identified demand‑side platform (DSP) user pain points in ad rejection due to business verification document submission difficulties; streamlined document submission during account creation, reducing ad rejection rate by 13 pp.\n- Defined business requirements and authored PRDs for contract termination workflows in Ad Agency Platform and Ad Admin, collaborating with designers and engineers to deliver features on time.\n- Analyzed UI/UX of competitor data‑management platforms (DMPs) and ran user surveys to enhance ad reports by adding campaign/group filters and a catalog ad report feature; led QA testing and post‑production monitoring, improving ad report performance by 50 %.\n- Resolved ad bulk registration abuse by implementing throttling, warning modals, and Slack alerts, cutting abuse cases by 59 %.\n\n\nAtommerce (Seoul, South Korea)\nProduct Owner Intern, Product Team (Feb 2023 – Jan 2024)\n- Managed end‑to‑end payment gateway approval process and localization for paid teletherapy service in Japan; coordinated app packaging and documentation, reducing approval time by 75 %.\n- Analyzed user flow and VoCs to improve acquisition, activation, and retention; drafted PRDs and wireframes for in‑app onboarding and community posting, boosting sign‑up conversion by 12 pp and engagement by 10 pp.\n- Owned SEO project, doubling organic web traffic and increasing impressions; handled weekly metric reporting and sprint ceremonies.\n\n\nDatbyeoul Education (Seoul, South Korea)\nProgram Manager (Sep 2019 – Jul 2024)\n- Collaborated with five private/public schools and local governments to design and implement innovative curricula and programs.\n- Supervised Model United Nations conferences for middle and high schools; drafted Rules of Procedure, guidebooks, and agendas; led regular student trainings.\n\n\n\nProject Experience\n\nVisual Presentation of Information and Digital Product Usability Across Cultures\nProject Lead (Aug 2024 – Present)\n- Designed and ran an experiment examining how analytic vs. holistic thinking styles affect usability perceptions across cultures, using the Analysis‑Holism Scale.\n- Created mock websites with varied visual complexity to measure satisfaction, efficiency, and performance under controlled conditions.\n- Performed power analysis in G*Power and used multiple linear regression to evaluate relationships between thinking style, visual complexity, and usability metrics.\n\n\n\nSkills\n\nLanguages: English & Korean (native)\n\nData Analysis: Python, R, SQL, Tableau, Superset, Google Analytics\n\nProductivity: Notion, Jira, Confluence, ClickUp, Microsoft Office (Excel, PPT, Word), Google Workspace, Qualtrics\n\nDesign: Figma, Adobe Creative Suite (InDesign, XD, Illustrator, Photoshop)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blair Kim",
    "section": "",
    "text": "I’m a senior at Pomona College majoring in Psychological Science set to graduate in May 2025.I’m proficient in Figma, R, SQL, and Tableau, and in my free time I love reading, gaming, drawing, and playing the piano."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "project5",
    "section": "",
    "text": "library(ggplot2)\nlibrary(sqldf)\nlibrary(tidyverse)\n\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nSHOW TABLES;\n\n\nDisplaying records 1 - 10\n\n\nTables_in_traffic\n\n\n\n\nar_little_rock_2020_04_01\n\n\naz_gilbert_2020_04_01\n\n\naz_mesa_2023_01_26\n\n\naz_statewide_2020_04_01\n\n\nca_anaheim_2020_04_01\n\n\nca_bakersfield_2020_04_01\n\n\nca_long_beach_2020_04_01\n\n\nca_los_angeles_2020_04_01\n\n\nca_oakland_2020_04_01\n\n\nca_san_bernardino_2020_04_01\n\n\n\n\n\nNumber of vehicle stops by race in big cities in California - San Francisco, Los Angeles, and San Diego\n\nSELECT count(*) AS vehicle_stops,\nsubject_race AS race\nFROM ca_san_francisco_2020_04_01    \nWHERE type = 'vehicular'\nGROUP BY subject_race\nORDER BY vehicle_stops DESC\n\n\nsf_vehicle_stops \n\n  vehicle_stops                   race\n1        372318                  white\n2        157684 asian/pacific islander\n3        152196                  black\n4        116014               hispanic\n5        106858                  other\n\n\n\nSELECT count(*) AS vehicle_stops,\nsubject_race AS race\nFROM ca_los_angeles_2020_04_01      \nWHERE type = 'vehicular'\nGROUP BY subject_race\nORDER BY vehicle_stops DESC\n\n\nla_vehicle_stops \n\n  vehicle_stops                   race\n1       1732357               hispanic\n2       1030083                  white\n3        881203                  black\n4        304175                  other\n5        187535 asian/pacific islander\n\n\n\nSELECT count(*) AS vehicle_stops,\nsubject_race AS race\nFROM ca_san_diego_2020_04_01        \nWHERE type = 'vehicular'\nGROUP BY subject_race\nORDER BY vehicle_stops DESC\n\n\nsd_vehicle_stops\n\n  vehicle_stops                   race\n1        162226                  white\n2        117083               hispanic\n3         42705                  black\n4         32541 asian/pacific islander\n5         27238                  other\n6          1234                   &lt;NA&gt;\n\n\n\nWere more POC drivers stopped by police in the past then now in LA?\n\nSELECT subject_race AS race, year(date) AS year, count(*) AS num_stops\nFROM ca_los_angeles_2020_04_01\nWHERE subject_race != 'white' AND date IS NOT NULL \nGROUP BY race, year(date)\nORDER BY year ASC\n\n\nla_POC_stops_by_year\n\n                     race year num_stops\n1                   other 2010     28694\n2                hispanic 2010    174642\n3                   black 2010     85514\n4  asian/pacific islander 2010     22319\n5                hispanic 2011    321677\n6                   black 2011    181602\n7                   other 2011     45593\n8  asian/pacific islander 2011     32555\n9                hispanic 2012    317404\n10                  black 2012    181566\n11                  other 2012     45966\n12 asian/pacific islander 2012     32114\n13                  other 2013     46555\n14                  black 2013    179500\n15               hispanic 2013    316371\n16 asian/pacific islander 2013     29269\n17               hispanic 2014    303600\n18                  black 2014    159642\n19                  other 2014     49573\n20 asian/pacific islander 2014     26939\n21               hispanic 2015    225807\n22                  black 2015    128514\n23 asian/pacific islander 2015     17748\n24                  other 2015     33286\n25                  black 2016    136117\n26               hispanic 2016    237468\n27                  other 2016     32344\n28 asian/pacific islander 2016     16215\n29                  black 2017    158470\n30               hispanic 2017    265883\n31                  other 2017     37479\n32 asian/pacific islander 2017     19369\n33                  other 2018     17852\n34                  black 2018     86959\n35               hispanic 2018    138973\n36 asian/pacific islander 2018      9033\n\n\n\n# visualize POC drivers stops by year\n\nla_POC_stops_by_year|&gt; \n  mutate(num_stops = as.numeric(num_stops)) |&gt; \n  ggplot(aes(x = year, y = num_stops, color = race)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"# of Police Stops of POC drivers in LA by year\", y = \"number of stops\", x = \"year\") \n\n\n\n\n\n\n\n\nAmong all racial groups, Hispanic drivers were most likely to be stopped by police in LA. It seems like there was a sharp increase in stops of POC drivers in 2011, but the numbers gradually decreased over time.\n\n\nWhat time of the day are people most likely to be arrested in San Jose and San Francisco?\n\nSELECT hour(time) AS hour, count(*) AS cases \nFROM ca_san_jose_2020_04_01 \nWHERE outcome = 'arrest'  \nGROUP BY hour(time)\nORDER BY cases DESC \n\n\n# visualize number of arrests by hour and year in San Jose\nsan_jose_arrests_by_time |&gt;\n  ggplot(aes(x = hour, y = cases)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"# of arrests by hour in San Jose\", y = \"number of arrests\", x = \"hour\") \n\n\n\n\n\n\n\n\n\nSELECT hour(time) AS hour, year(date) AS year, count(*) AS cases \nFROM ca_san_jose_2020_04_01 \nWHERE outcome = 'arrest'  \nGROUP BY hour(time), year(date)\nORDER BY cases DESC \n\n\n# visualize number of arrests by hour and year in San Jose\nsan_jose_arrests_by_time |&gt;\n  ggplot(aes(x = hour, y = cases, group = year)) +\n  geom_line(aes(color = year)) +\n  theme_minimal() +\n  labs(title = \"# of arrests by hour in San Jose over time\", y = \"number of arrests\", x = \"hour\") \n\n\n\n\n\n\n\n\n\nSELECT hour(time) AS hour, count(*) AS cases \nFROM ca_san_francisco_2020_04_01    \nWHERE outcome = 'arrest'  \nGROUP BY hour(time)\nORDER BY cases DESC \n\n\n# visualize number of arrests by hour and year in San Jose\nsan_francisco_arrests_by_time |&gt;\n  ggplot(aes(x = hour, y = cases)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"# of arrests by hour in San Francisco\", y = \"number of arrests\", x = \"hour\") \n\n\n\n\n\n\n\n\n\nselect hour(time) as hour, year(date) AS year, count(*) as cases \nfrom ca_san_francisco_2020_04_01    \nwhere outcome = 'arrest'  \ngroup by hour(time), year(date)\norder by cases desc \n\n\n# visualize number of arrests by hour in San Francisco\nsf_arrests_by_time |&gt;\n  ggplot(aes(x = hour, y = cases, group = year)) +\n  geom_line(aes(color = year)) +\n  theme_minimal() +\n  labs(title = \"# of arrests by hour in San Francisco over time\", y = \"number of arrests\", x = \"hour\")\n\n\n\n\n\n\n\n\nIn San Jose, around 9 AM is the peak time for getting arrested on the road. In San Francisco, around 5pm and midnight is when people get arrested most often. The trend looks consistent over time, but there were more arrests overall back in 2013 (dark blue line) than recently in 2018 (light blue line).\n```\n\n\nReferences\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Project 4",
    "section": "",
    "text": "Facebook’s Emotion Manipulation Experiment - Implications in Data Ethics\nIn 2014, Facebook (currently Meta) conducted a large-scale psychological experiment on 689,003 users, manipulating their news feed to see if emotions displayed on social media are contagious (Kramer, Guillory & Hancock, 2014). Emotional contagion has traditionally been studied in real-life, in-person settings, notably in the Framingham Heart Study where researchers conducted a 20-year-long longitudinal research on 4739 individuals and found evidence that happiness is a collective phenomenon (Fowler & Christakis, 2008). Although Facebook’s emotion contagion study might be pioneering in that it found patterns of emotion contagion in digital space, the ethical integrity of this study is questionable in many aspects.\n\nAbout the Study\nIn this study, researchers randomly selected and were divided into three groups - positive emotion condition, negative emotion condition, and control. Users’ feed in the two emotion conditions were manipulated, showing posts that contained either positive or negative emotions. Then, posts that users in each condition were analyzed to measure emotionality. Results showed that the manipulation worked - for people who had less positive content in their News Feed showed more negative emotions in their status update and vice versa, providing evidence for emotion contagion in online social network.\n\n\nData Ethics Concerns\n\nConsent structure\nThe consent structure of this study is probably the most troubling part. The study claims that by using systems that did not allow researchers to see any text, it was “consistent with Facebook’s Data Use Policy, to which all users agree prior to creating an account on Facebook” and constituted consent for this research.\nHowever, it is hard to see such a procedure to be equivalent of standard consent procedure. In psychological research, participants are informed of what the study is about, potential harm the experiment might inflict, can leave the study at any point, and are debriefed and provided with any resources that might need to compensate for any emotional distress that might have been caused by the study. In that sense, participation to this study was not voluntary, users had no idea that their feed was being manipulated for research purposes. A Forbes article found that the research was conducted four months before adding “research” to their Data Use Policy, and users under the age of 18 may have been included in this study as there was no age filter set on the study.\nAnonymity of data\nAccording to the study, participants were randomly selected based on their user ID, and there was no other information about the participants reported. It seems like it’s safe to assume that data was anonymous in this research. Whether users are identifiable with their user IDs in Facebook’s on database would be a different issue and also depend on who has access to the information. Considering that one of the authors is affiliated with Facebook’s Core Data Science Team, if users are identifiable with user ID in Facebook’s own database, it would be a problem. According to Facebook’s help article, user ID is a string of numbers that does not personally identify a user but “does connect to your Facebook profile and specific chats on Messenger”, so it does seem like user ID is somewhat identifiable and the data collected in this study might not be anonymous at all.\nRepresentativeness of the sample\nSince users were randomly selected based on their user ID with no other demographic information provided, the study used a strictly random sample. However, since there’s is no information about the participants such as age, gender, and race, we don’t know if the sample excluded people from certain demographics. Also, the reprensetativeness of the sample is limited to Facebook users over time period, which is different from all people.\nAccessibility of the data\nThe data was not made publicly available, which was probably the right thing to do considering privacy concerns. However, inaccessibility of the data we can’t ensure reproducibility of the results and make it hard for other researchers to verify the accuracy of the data, analyses, and conclusions. The inaccessibility might also hinder further research to build upon the findings of the given study.\n\n\n\nReferences\n\nFowler, J. H., & Christakis, N. A. (2008). Dynamic spread of happiness in a large social network: longitudinal analysis over 20 years in the Framingham Heart Study. BMJ (Clinical research ed.), 337, a2338. https://doi.org/10.1136/bmj.a2338\nMcNeal, G. S. (2014, July 1). Facebook manipulated user news feeds to create emotional responses. Forbes. https://www.forbes.com/sites/gregorymcneal/2014/06/28/facebook-manipulated-user-news-feeds-to-create-emotional-contagion/\nKramer, A. D., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences of the United States of America, 111(24), 8788–8790. https://doi.org/10.1073/pnas.1320040111\nHow usernames and user ids are used on Facebook Profiles: Facebook help center. How usernames and user IDs are used on Facebook profiles | Facebook Help Center. (n.d.). https://www.facebook.com/help/211813265517027/"
  },
  {
    "objectID": "project6.html",
    "href": "project6.html",
    "title": "Project 6",
    "section": "",
    "text": "Added brief introduction, image, and button links to social media pages.\n\n\n\nAdded my resume.\n\n\n\n\n\n\nAdded YAML code to hide messages and warnings.\nAdded a brief description of the data.\n\n\n\n\n\nAdded a brief description of the data.\n\n\n\n\n\nEdited page title.\nAdded a brief description of the data.\n\n\n\n\n\nMoved text from code blocks to plain text so that it’s easier to read.\nEdited introduction and conclusion to avoid causal claims and claims like “the null hypothesis is true.”\n\n\n\n\n\nAdded additional analysis under Data Ethics concern - Representativeness and Accessibility.\n\n\n\n\n\nEdited SQL functions in all caps.\nAdded a graph that shows the number of arrest by time of the day in different years."
  },
  {
    "objectID": "project6.html#about",
    "href": "project6.html#about",
    "title": "Project 6",
    "section": "",
    "text": "Added my resume."
  },
  {
    "objectID": "project6.html#data-viz",
    "href": "project6.html#data-viz",
    "title": "Project 6",
    "section": "",
    "text": "Added YAML code to hide messages and warnings.\nAdded a brief description of the data.\n\n\n\n\n\nAdded a brief description of the data.\n\n\n\n\n\nEdited page title.\nAdded a brief description of the data.\n\n\n\n\n\nMoved text from code blocks to plain text so that it’s easier to read.\nEdited introduction and conclusion to avoid causal claims and claims like “the null hypothesis is true.”\n\n\n\n\n\nAdded additional analysis under Data Ethics concern - Representativeness and Accessibility.\n\n\n\n\n\nEdited SQL functions in all caps.\nAdded a graph that shows the number of arrest by time of the day in different years."
  },
  {
    "objectID": "theoffice.html",
    "href": "theoffice.html",
    "title": "The Office",
    "section": "",
    "text": "About the data\nThis data contains lines from every season of The Office.\n\n\nAnalysis\n\n# Get the Data\n\nthe_office_lines &lt;- read.csv(\"the-office_lines.csv\")\n\n\n# install packages \nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Who says \"hey\" the most in the Office by season? \n\n\nmain_characters &lt;- c('Pam', 'Jim', 'Michael', 'Dwight', 'Ryan', 'Andy')\n\n\nhey_by_character &lt;- the_office_lines |&gt; \n  mutate(line_lower = tolower(Line)) |&gt; \n  filter(str_detect(line_lower, \"hey\")) |&gt; \n  group_by(Season, Character) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt;\n  filter(Character %in% main_characters)\n\nggplot(hey_by_character, aes(x = Season, y = count, color = Character)) + \n  geom_point()+\n  labs(x = 'Season', y = 'Count',\n       title = 'Who says \"hey\" the most in each season of The Office?') +\n    scale_x_continuous(breaks=c(1:10))\n\n\n\n\n\n\n\n\nMichael says “hey” the most, and he really peaked in season 2 to 6.\n\n# Damn it Who? \n\nthe_office_lines_lower &lt;- \n  mutate(the_office_lines, line_lower = tolower(Line))\n\n\ndamn_it &lt;- \"damn it*\\\\s*(\\\\w+)\"\n\nthe_office_damnit &lt;- the_office_lines_lower |&gt;\n  mutate(character_damn_it = str_match(line_lower, damn_it) [ ,2])  \n\ndamn_it_counts &lt;- the_office_damnit |&gt; \n  filter(!is.na(character_damn_it)) |&gt;\n  group_by(Character) |&gt;\n  summarize(count=n()) |&gt;\n  slice_max(count, n = 10)\n\ndamn_it_counts |&gt;\n  ggplot(aes(x = Character, y = count)) +\n  geom_point() +\n  labs(\n    title = \"Damn it who?\",\n    x = \"Character\",\n    y = \"Count\",\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDamn it, Dwight! Dwight is the character whose name is mentioned the most after the phrase “damn it”.\nSource: https://www.kaggle.com/datasets/fabriziocominetti/the-office-lines/data"
  },
  {
    "objectID": "simpsons.html",
    "href": "simpsons.html",
    "title": "Simpsons",
    "section": "",
    "text": "About the data\nThis dataset is from tidytuesday and contains the character, locations, episode details, and script lines for about 600 Simpsons episodes.\n\ninstall.packages(\"tidytuesdayR\", repos = \"https://cloud.r-project.org/\")\n\n\nThe downloaded binary packages are in\n    /var/folders/vg/12r57nf51593b_d4zr1kvcgw0000gn/T//RtmpOWC1Ga/downloaded_packages\n\nlibrary(\"tidytuesdayR\")\n\n\n###_____________________________________________________________________________\n### The Simpson's data!\n### Script to clean the data sourced from Kaggle\n###_____________________________________________________________________________\n\n# packages\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(withr)\n\n# Define the metadata URL and fetch it\nmetadata_url &lt;- \"www.kaggle.com/datasets/prashant111/the-simpsons-dataset/croissant/download\"\nresponse &lt;- httr::GET(metadata_url)\n\n# Ensure the request succeeded\nif (httr::http_status(response)$category != \"Success\") {\n  stop(\"Failed to fetch metadata.\")\n}\n\n# Parse the metadata\nmetadata &lt;- httr::content(response, as = \"parsed\", type = \"application/json\")\n\n# Locate the ZIP file URL\ndistribution &lt;- metadata$distribution\nzip_url &lt;- NULL\n\nfor (file in distribution) {\n  if (file$encodingFormat == \"application/zip\") {\n    zip_url &lt;- file$contentUrl\n    break\n  }\n}\n\nif (is.null(zip_url)) {\n  stop(\"No ZIP file URL found in the metadata.\")\n}\n\n# Download the ZIP file. We'll use the withr package to make sure the downloaded\n# files get cleaned up when we're done.\ntemp_file &lt;- withr::local_tempfile(fileext = \".zip\") \nutils::download.file(zip_url, temp_file, mode = \"wb\")\n\n# Unzip and read the CSV\nunzip_dir &lt;- withr::local_tempdir()\nutils::unzip(temp_file, exdir = unzip_dir)\n\n# Locate the CSV file within the extracted contents\ncsv_file &lt;- list.files(unzip_dir, pattern = \"\\\\.csv$\", full.names = TRUE)\n\nif (length(csv_file) == 0) {\n  stop(\"No CSV file found in the unzipped contents.\")\n}\n\n# Read the CSV into a dataframe\nsimpsons_characters &lt;- read_csv(csv_file[1])\nsimpsons_episodes &lt;- read_csv(csv_file[2])\nsimpsons_locations &lt;- read_csv(csv_file[3])\nsimpsons_script_lines &lt;- read_csv(csv_file[4])\n\n# Step 5: Explore the data\nglimpse(simpsons_characters)\n\nRows: 6,722\nColumns: 4\n$ id              &lt;dbl&gt; 7, 12, 13, 16, 20, 24, 26, 27, 29, 30, 34, 35, 36, 37,…\n$ name            &lt;chr&gt; \"Children\", \"Mechanical Santa\", \"Tattoo Man\", \"DOCTOR …\n$ normalized_name &lt;chr&gt; \"children\", \"mechanical santa\", \"tattoo man\", \"doctor …\n$ gender          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\nglimpse(simpsons_episodes)\n\nRows: 600\nColumns: 14\n$ id                     &lt;dbl&gt; 10, 12, 14, 17, 19, 21, 23, 26, 28, 30, 32, 35,…\n$ image_url              &lt;chr&gt; \"http://static-media.fxx.com/img/FX_Networks_-_…\n$ imdb_rating            &lt;dbl&gt; 7.4, 8.3, 8.2, 8.1, 8.0, 8.4, 7.8, 8.0, 8.2, 7.…\n$ imdb_votes             &lt;dbl&gt; 1511, 1716, 1638, 1457, 1366, 1522, 1340, 1329,…\n$ number_in_season       &lt;dbl&gt; 10, 12, 1, 4, 6, 8, 10, 13, 15, 17, 19, 22, 2, …\n$ number_in_series       &lt;dbl&gt; 10, 12, 14, 17, 19, 21, 23, 26, 28, 30, 32, 35,…\n$ original_air_date      &lt;date&gt; 1990-03-25, 1990-04-29, 1990-10-11, 1990-11-01…\n$ original_air_year      &lt;dbl&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1991, 1991,…\n$ production_code        &lt;chr&gt; \"7G10\", \"7G12\", \"7F03\", \"7F01\", \"7F08\", \"7F06\",…\n$ season                 &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,…\n$ title                  &lt;chr&gt; \"Homer's Night Out\", \"Krusty Gets Busted\", \"Bar…\n$ us_viewers_in_millions &lt;dbl&gt; 30.3, 30.4, 33.6, 26.1, 25.4, 26.2, 24.8, 26.2,…\n$ video_url              &lt;chr&gt; \"http://www.simpsonsworld.com/video/27519750787…\n$ views                  &lt;dbl&gt; 50816, 62561, 59575, 64959, 50691, 57605, 56486…\n\nglimpse(simpsons_locations)\n\nRows: 4,459\nColumns: 3\n$ id              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ name            &lt;chr&gt; \"Street\", \"Car\", \"Springfield Elementary School\", \"Aud…\n$ normalized_name &lt;chr&gt; \"street\", \"car\", \"springfield elementary school\", \"aud…\n\nglimpse(simpsons_script_lines)\n\nRows: 158,271\nColumns: 13\n$ id                 &lt;dbl&gt; 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 955…\n$ episode_id         &lt;dbl&gt; 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,…\n$ number             &lt;dbl&gt; 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 2…\n$ raw_text           &lt;chr&gt; \"Miss Hoover: No, actually, it was a little of both…\n$ timestamp_in_ms    &lt;dbl&gt; 848000, 856000, 856000, 864000, 864000, 877000, 881…\n$ speaking_line      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FAL…\n$ character_id       &lt;dbl&gt; 464, 9, 464, 9, 40, 38, 40, 8, NA, 9, 469, 9, 469, …\n$ location_id        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 374, 374, 374, 374, 374, 37…\n$ raw_character_text &lt;chr&gt; \"Miss Hoover\", \"Lisa Simpson\", \"Miss Hoover\", \"Lisa…\n$ raw_location_text  &lt;chr&gt; \"Springfield Elementary School\", \"Springfield Eleme…\n$ spoken_words       &lt;chr&gt; \"No, actually, it was a little of both. Sometimes w…\n$ normalized_text    &lt;chr&gt; \"no actually it was a little of both sometimes when…\n$ word_count         &lt;dbl&gt; 31, 3, 22, 5, 33, 8, 1, 5, NA, 4, 19, 8, 10, 9, 19,…\n\n###_____________________________________________________________________________\n# Problems with the Data!\n\n# The script lines are of great interest, but it is a larger file, too big\n# for Tidy Tuesday.  We need to reduce the size of the file so we can use all\n# the files together for a more robust analysis.\n# Let's filter episodes down to the years 2010-2016, and then only select\n# the script lines that correspond with those episodes.\n\n###_____________________________________________________________________________\n\n# filter episodes to include 2010+\nsimpsons_episodes &lt;- simpsons_episodes |&gt; \n  dplyr::filter(original_air_year &gt;= 2010)\n\n# filter script lines to only include lines for these episodes\nsimpsons_script_lines &lt;- simpsons_script_lines |&gt; \n  dplyr::semi_join(simpsons_episodes, by = c(\"episode_id\" = \"id\"))\n\n\n\nAnalysis\n\n# simpsons ratings by seasons\n ggplot(simpsons_episodes, aes(x = original_air_year, y = imdb_rating)) +\n       geom_point() +\n      geom_smooth(se = FALSE) +\n      labs(\n        x = \"air year\",\n        y = \"rating\",\n        title = \"Simpsons rating by year\")\n\n\n\n\n\n\n\n\n(Harmon and Hughes 2024)\n\n\n\n\n\n\nNote\n\n\n\nOriginal data from https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset\n\n\n\n\n\n\n\nReferences\n\nHarmon, Jon, and Ellis Hughes. 2024. “tidytuesdayR: Access the Weekly ’TidyTuesday’ Project Dataset.” https://CRAN.R-project.org/package=tidytuesdayR."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Salary Gender Gap",
    "section": "",
    "text": "Introduction\nIn this project, I plan to test if the gender gap in salary is real, or just by random chance. In order to do this, I found a dataset on Kaggle that has information about the salaries of employees at a company with their gender, age, years of experience, education level, and job title.\n\n\nAnalysis\n\n# packages\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n# import dataset \nsalary &lt;- read.csv(\"Salary Data.csv\")\n\n# data cleaning \nsalary_clean &lt;- salary |&gt; filter(!is.na(Age))\n\n\n# averages and median salary between men and women\n\nsalary_clean |&gt;\n  group_by(Gender) |&gt;\n  summarize(ave_salary = mean(Salary),\n            med_salary = median(Salary)) |&gt;\n  summarize(ave_diff = diff(ave_salary),\n            med_diff = diff(med_salary))\n\n# A tibble: 1 × 2\n  ave_diff med_diff\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    6857.     7500\n\n\n\n# generate a null sampling distribution \nperm_data &lt;- function(rep, data){\n  salary_clean |&gt; \n    select(Gender, Salary) |&gt; \n    mutate(salary_perm = sample(Salary, replace = FALSE)) |&gt; \n    group_by(Gender) |&gt; \n    summarize(obs_ave = mean(Salary),\n              obs_med = median(Salary),\n              perm_ave = mean(salary_perm),\n              perm_med = median(salary_perm)) |&gt; \n    summarize(obs_ave_diff = diff(obs_ave),\n              obs_med_diff = diff(obs_med),\n              perm_ave_diff = diff(perm_ave),\n              perm_med_diff = diff(perm_med),\n              rep = rep)\n}\n\n\nmap(c(1:10), perm_data, data = Salary) |&gt; \n  list_rbind()\n\n# A tibble: 10 × 5\n   obs_ave_diff obs_med_diff perm_ave_diff perm_med_diff   rep\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n 1        6857.         7500        -8829.         -5000     1\n 2        6857.         7500         1110.             0     2\n 3        6857.         7500         4758.             0     3\n 4        6857.         7500         -344.             0     4\n 5        6857.         7500         3205.             0     5\n 6        6857.         7500        -8342.         -7500     6\n 7        6857.         7500         4275.             0     7\n 8        6857.         7500          623.        -10000     8\n 9        6857.         7500          462.             0     9\n10        6857.         7500         5027.          2500    10\n\n\n\n# visualize the null sampling distribution\nset.seed(1108)\n\nperm_stats &lt;- map(c(1:500), perm_data, data = salary_clean) |&gt;\n  list_rbind() \n\n# average\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n# median\nperm_stats |&gt;\n  ggplot(aes(x = perm_med_diff)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n# p_value\nperm_stats |&gt; \n  summarize(p_val_ave = mean(perm_ave_diff &gt; obs_ave_diff),\n            p_val_med = mean(perm_med_diff &gt; obs_med_diff))\n\n# A tibble: 1 × 2\n  p_val_ave p_val_med\n      &lt;dbl&gt;     &lt;dbl&gt;\n1     0.078      0.08\n\n\n\n\nConclusion\nThe observed differences are consistent with the distribution of differences in the null sampling distribution. Based on the p-values (0.078 for the average salary and 0.08 for the median salary), we fail to reject the null hypothesis. This means there isn’t enough evidence to claim that, in the population, the average salary of male employees is larger than that of female employees. Similarly, we cannot claim that the median salary of male employees is larger than the median salary of female employees. Both p-values suggest we do not have enough evidence to rule out random chance.\nSource: Kiattisak Rattanaporn, Salary Prediction dataset from https://www.kaggle.com/datasets/rkiattisak/salaly-prediction-for-beginer?resource=download"
  },
  {
    "objectID": "valentine.html",
    "href": "valentine.html",
    "title": "valentine",
    "section": "",
    "text": "About the data\nThis data is by NRF survey about how consumers plan to celebrate Valentine’s Day annually for over a decade. It provides a demographic breakdown of total spending, average spending, types of gifts planned and spending per type of gift.\n\n\nAnalysis\n\n# tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n\nhistorical_spending &lt;- tuesdata$historical_spending\ngifts_age &lt;- tuesdata$gifts_age\ngifts_gender &lt;- tuesdata$gifts_gender\n\n\n# packages\nlibrary(tidyverse)\n\n\n# popular gift on valentine's day by year \n\n ggplot(historical_spending, aes(x = Year)) +\n  geom_line(aes(y = Flowers, color = \"Flowers\")) +\n  geom_line(aes(y = Candy, color = \"Candy\")) +\n  geom_line(aes(y = Jewelry, color = \"Jewelry\")) +\n  labs(\n    x = \"Year\",\n    y = \"Average Percent Spending\",\n    title = \"Popular gift on Valentine's Day\",\n    color = \"Gift Type\")\n\n\n\n\n\n\n\n\n(Harmon and Hughes 2024)\n\n\n\n\n\n\nNote\n\n\n\nOriginal data from https://www.kaggle.com/datasets/infinator/happy-valentines-day-2022\n\n\n\n\n\n\n\nReferences\n\nHarmon, Jon, and Ellis Hughes. 2024. “tidytuesdayR: Access the Weekly ’TidyTuesday’ Project Dataset.” https://CRAN.R-project.org/package=tidytuesdayR."
  }
]