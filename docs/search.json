[
  {
    "objectID": "valentine.html",
    "href": "valentine.html",
    "title": "valentine",
    "section": "",
    "text": "# tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n\n---- Compiling #TidyTuesday Information for 2024-02-13 ----\n--- There are 3 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 3: \"historical_spending.csv\"\n  2 of 3: \"gifts_age.csv\"\n  3 of 3: \"gifts_gender.csv\"\n\nhistorical_spending &lt;- tuesdata$historical_spending\ngifts_age &lt;- tuesdata$gifts_age\ngifts_gender &lt;- tuesdata$gifts_gender\n\n\n# packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# popular gift on valentine's day by year \n\n ggplot(historical_spending, aes(x = Year)) +\n  geom_line(aes(y = Flowers, color = \"Flowers\")) +\n  geom_line(aes(y = Candy, color = \"Candy\")) +\n  geom_line(aes(y = Jewelry, color = \"Jewelry\")) +\n  labs(\n    x = \"Year\",\n    y = \"Average Percent Spending\",\n    title = \"Popular gift on Valentine's Day\",\n    color = \"Gift Type\")\n\n\n\n\n\n\n\n\n(Harmon and Hughes 2024)\n\n\n\n\n\n\nNote\n\n\n\nOriginal data from https://www.kaggle.com/datasets/infinator/happy-valentines-day-2022\n\n\n\n\n\n\nReferences\n\nHarmon, Jon, and Ellis Hughes. 2024. “tidytuesdayR: Access the Weekly ’TidyTuesday’ Project Dataset.” https://CRAN.R-project.org/package=tidytuesdayR."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blairkim1108.github.io",
    "section": "",
    "text": "This is a Quarto website."
  },
  {
    "objectID": "theoffice.html",
    "href": "theoffice.html",
    "title": "theoffice",
    "section": "",
    "text": "# Get the Data\n\nthe_office_lines &lt;- read.csv(\"the-office_lines.csv\")\n\n\n# install packages \nlibrary(stringr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Who says \"hey\" the most in the Office by season? \n\n\nmain_characters &lt;- c('Pam', 'Jim', 'Michael', 'Dwight', 'Ryan', 'Andy')\n\n\nhey_by_character &lt;- the_office_lines |&gt; \n  mutate(line_lower = tolower(Line)) |&gt; \n  filter(str_detect(line_lower, \"hey\")) |&gt; \n  group_by(Season, Character) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt;\n  filter(Character %in% main_characters)\n\nggplot(hey_by_character, aes(x = Season, y = count, color = Character)) + \n  geom_point()+\n  labs(x = 'Season', y = 'Count',\n       title = 'Who says \"hey\" the most in each season of The Office?') +\n    scale_x_continuous(breaks=c(1:10))\n\n\n\n\n\n\n\n\nMichael says “hey” the most, and he really peaked in season 2 to 6.\n\n# Damn it Who? \n\nthe_office_lines_lower &lt;- \n  mutate(the_office_lines, line_lower = tolower(Line))\n\n\ndamn_it &lt;- \"damn it*\\\\s*(\\\\w+)\"\n\nthe_office_damnit &lt;- the_office_lines_lower |&gt;\n  mutate(character_damn_it = str_match(line_lower, damn_it) [ ,2])  \n\ndamn_it_counts &lt;- the_office_damnit |&gt; \n  filter(!is.na(character_damn_it)) |&gt;\n  group_by(Character) |&gt;\n  summarize(count=n()) |&gt;\n  slice_max(count, n = 10)\n\ndamn_it_counts |&gt;\n  ggplot(aes(x = Character, y = count)) +\n  geom_point() +\n  labs(\n    title = \"Damn it who?\",\n    x = \"Character\",\n    y = \"Count\",\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDamn it, Dwight! Dwight is the character whose name is mentioned the most after the phrase “damn it”.\nSource: https://www.kaggle.com/datasets/fabriziocominetti/the-office-lines/data"
  },
  {
    "objectID": "simpsons.html",
    "href": "simpsons.html",
    "title": "simpsons",
    "section": "",
    "text": "install.packages(\"tidytuesdayR\", repos = \"https://cloud.r-project.org/\")\n\n\nThe downloaded binary packages are in\n    /var/folders/vg/12r57nf51593b_d4zr1kvcgw0000gn/T//RtmpdIRVGl/downloaded_packages\n\nlibrary(\"tidytuesdayR\")\n\n\n###_____________________________________________________________________________\n### The Simpson's data!\n### Script to clean the data sourced from Kaggle\n###_____________________________________________________________________________\n\n# packages\nlibrary(httr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(withr)\n\n# Define the metadata URL and fetch it\nmetadata_url &lt;- \"www.kaggle.com/datasets/prashant111/the-simpsons-dataset/croissant/download\"\nresponse &lt;- httr::GET(metadata_url)\n\n# Ensure the request succeeded\nif (httr::http_status(response)$category != \"Success\") {\n  stop(\"Failed to fetch metadata.\")\n}\n\n# Parse the metadata\nmetadata &lt;- httr::content(response, as = \"parsed\", type = \"application/json\")\n\n# Locate the ZIP file URL\ndistribution &lt;- metadata$distribution\nzip_url &lt;- NULL\n\nfor (file in distribution) {\n  if (file$encodingFormat == \"application/zip\") {\n    zip_url &lt;- file$contentUrl\n    break\n  }\n}\n\nif (is.null(zip_url)) {\n  stop(\"No ZIP file URL found in the metadata.\")\n}\n\n# Download the ZIP file. We'll use the withr package to make sure the downloaded\n# files get cleaned up when we're done.\ntemp_file &lt;- withr::local_tempfile(fileext = \".zip\") \nutils::download.file(zip_url, temp_file, mode = \"wb\")\n\n# Unzip and read the CSV\nunzip_dir &lt;- withr::local_tempdir()\nutils::unzip(temp_file, exdir = unzip_dir)\n\n# Locate the CSV file within the extracted contents\ncsv_file &lt;- list.files(unzip_dir, pattern = \"\\\\.csv$\", full.names = TRUE)\n\nif (length(csv_file) == 0) {\n  stop(\"No CSV file found in the unzipped contents.\")\n}\n\n# Read the CSV into a dataframe\nsimpsons_characters &lt;- read_csv(csv_file[1])\n\nRows: 6722 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, normalized_name, gender\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsimpsons_episodes &lt;- read_csv(csv_file[2])\n\nRows: 600 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): image_url, production_code, title, video_url\ndbl  (9): id, imdb_rating, imdb_votes, number_in_season, number_in_series, o...\ndate (1): original_air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsimpsons_locations &lt;- read_csv(csv_file[3])\n\nRows: 4459 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): name, normalized_name\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsimpsons_script_lines &lt;- read_csv(csv_file[4])\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 158271 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): raw_text, raw_character_text, raw_location_text, spoken_words, norm...\ndbl (7): id, episode_id, number, timestamp_in_ms, character_id, location_id,...\nlgl (1): speaking_line\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Step 5: Explore the data\nglimpse(simpsons_characters)\n\nRows: 6,722\nColumns: 4\n$ id              &lt;dbl&gt; 7, 12, 13, 16, 20, 24, 26, 27, 29, 30, 34, 35, 36, 37,…\n$ name            &lt;chr&gt; \"Children\", \"Mechanical Santa\", \"Tattoo Man\", \"DOCTOR …\n$ normalized_name &lt;chr&gt; \"children\", \"mechanical santa\", \"tattoo man\", \"doctor …\n$ gender          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\nglimpse(simpsons_episodes)\n\nRows: 600\nColumns: 14\n$ id                     &lt;dbl&gt; 10, 12, 14, 17, 19, 21, 23, 26, 28, 30, 32, 35,…\n$ image_url              &lt;chr&gt; \"http://static-media.fxx.com/img/FX_Networks_-_…\n$ imdb_rating            &lt;dbl&gt; 7.4, 8.3, 8.2, 8.1, 8.0, 8.4, 7.8, 8.0, 8.2, 7.…\n$ imdb_votes             &lt;dbl&gt; 1511, 1716, 1638, 1457, 1366, 1522, 1340, 1329,…\n$ number_in_season       &lt;dbl&gt; 10, 12, 1, 4, 6, 8, 10, 13, 15, 17, 19, 22, 2, …\n$ number_in_series       &lt;dbl&gt; 10, 12, 14, 17, 19, 21, 23, 26, 28, 30, 32, 35,…\n$ original_air_date      &lt;date&gt; 1990-03-25, 1990-04-29, 1990-10-11, 1990-11-01…\n$ original_air_year      &lt;dbl&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1991, 1991,…\n$ production_code        &lt;chr&gt; \"7G10\", \"7G12\", \"7F03\", \"7F01\", \"7F08\", \"7F06\",…\n$ season                 &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,…\n$ title                  &lt;chr&gt; \"Homer's Night Out\", \"Krusty Gets Busted\", \"Bar…\n$ us_viewers_in_millions &lt;dbl&gt; 30.3, 30.4, 33.6, 26.1, 25.4, 26.2, 24.8, 26.2,…\n$ video_url              &lt;chr&gt; \"http://www.simpsonsworld.com/video/27519750787…\n$ views                  &lt;dbl&gt; 50816, 62561, 59575, 64959, 50691, 57605, 56486…\n\nglimpse(simpsons_locations)\n\nRows: 4,459\nColumns: 3\n$ id              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ name            &lt;chr&gt; \"Street\", \"Car\", \"Springfield Elementary School\", \"Aud…\n$ normalized_name &lt;chr&gt; \"street\", \"car\", \"springfield elementary school\", \"aud…\n\nglimpse(simpsons_script_lines)\n\nRows: 158,271\nColumns: 13\n$ id                 &lt;dbl&gt; 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 955…\n$ episode_id         &lt;dbl&gt; 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,…\n$ number             &lt;dbl&gt; 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 2…\n$ raw_text           &lt;chr&gt; \"Miss Hoover: No, actually, it was a little of both…\n$ timestamp_in_ms    &lt;dbl&gt; 848000, 856000, 856000, 864000, 864000, 877000, 881…\n$ speaking_line      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FAL…\n$ character_id       &lt;dbl&gt; 464, 9, 464, 9, 40, 38, 40, 8, NA, 9, 469, 9, 469, …\n$ location_id        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 374, 374, 374, 374, 374, 37…\n$ raw_character_text &lt;chr&gt; \"Miss Hoover\", \"Lisa Simpson\", \"Miss Hoover\", \"Lisa…\n$ raw_location_text  &lt;chr&gt; \"Springfield Elementary School\", \"Springfield Eleme…\n$ spoken_words       &lt;chr&gt; \"No, actually, it was a little of both. Sometimes w…\n$ normalized_text    &lt;chr&gt; \"no actually it was a little of both sometimes when…\n$ word_count         &lt;dbl&gt; 31, 3, 22, 5, 33, 8, 1, 5, NA, 4, 19, 8, 10, 9, 19,…\n\n###_____________________________________________________________________________\n# Problems with the Data!\n\n# The script lines are of great interest, but it is a larger file, too big\n# for Tidy Tuesday.  We need to reduce the size of the file so we can use all\n# the files together for a more robust analysis.\n# Let's filter episodes down to the years 2010-2016, and then only select\n# the script lines that correspond with those episodes.\n\n###_____________________________________________________________________________\n\n# filter episodes to include 2010+\nsimpsons_episodes &lt;- simpsons_episodes |&gt; \n  dplyr::filter(original_air_year &gt;= 2010)\n\n# filter script lines to only include lines for these episodes\nsimpsons_script_lines &lt;- simpsons_script_lines |&gt; \n  dplyr::semi_join(simpsons_episodes, by = c(\"episode_id\" = \"id\"))\n\n\n# simpsons ratings by seasons\n ggplot(simpsons_episodes, aes(x = original_air_year, y = imdb_rating)) +\n       geom_point() +\n      geom_smooth(se = FALSE) +\n      labs(\n        x = \"air year\",\n        y = \"rating\",\n        title = \"Simpsons rating by year\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n(Harmon and Hughes 2024)\n\n\n\n\n\n\nNote\n\n\n\nOriginal data from https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset\n\n\n\n\n\n\nReferences\n\nHarmon, Jon, and Ellis Hughes. 2024. “tidytuesdayR: Access the Weekly ’TidyTuesday’ Project Dataset.” https://CRAN.R-project.org/package=tidytuesdayR."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]